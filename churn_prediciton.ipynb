{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cecf68",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508520f3",
   "metadata": {},
   "source": [
    "- Customer retention is one of the primary KPI for companies with a subscription-based business model. Competition is tough particularly in the SaaS market where customers are free to choose from plenty of providers. One bad experience and customer may just move to the competitor resulting in customer churn.\n",
    "\n",
    "- Customer churn is the percentage of customers that stopped using your companyâ€™s product or service during a certain time frame. One of the ways to calculate a churn rate is to divide the number of customers lost during a given time interval by the number of active customers at the beginning of the period.\n",
    "\n",
    "- Predicting customer churn is a challenging but extremely important business problem especially in industries where the cost of customer acquisition (CAC) is high such as technology, telecom, finance, etc. The ability to predict that a particular customer is at a high risk of churning, while there is still time to do something about it, represents a huge additional potential revenue source for companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4295b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep and Visuals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#set max rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d978f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_null_values(df, column_name, replace_with=None, drop_threshold=0.05):\n",
    "    # Replace null values with a given word\n",
    "    if replace_with is not None:\n",
    "        df[column_name].fillna(replace_with, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Drop rows with null values if they are less than the threshold\n",
    "    if drop_threshold > 0:\n",
    "        null_count = df[column_name].isnull().sum()\n",
    "        total_rows = len(df)\n",
    "        null_percentage = null_count / total_rows\n",
    "        if null_percentage < drop_threshold:\n",
    "            df.dropna(subset=[column_name], inplace=True)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c42569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_duplicates(df, column_name = None, keep_first=True):\n",
    "    if keep_first:\n",
    "        df.drop_duplicates( keep='first', inplace=True)\n",
    "    else:\n",
    "        df.drop_duplicates( inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8536690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame\n",
    "df = pd.read_csv(\"October_Churn_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b1ac18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "      <th>month</th>\n",
       "      <th>Churn_1</th>\n",
       "      <th>Churn_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>44600062</td>\n",
       "      <td>2103807459595387724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>35.79</td>\n",
       "      <td>541312140</td>\n",
       "      <td>72d76fde-8bb3-4e00-8c23-a032dfed738c</td>\n",
       "      <td>Oct</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-01 00:00:15 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>44600062</td>\n",
       "      <td>2103807459595387724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>35.79</td>\n",
       "      <td>541312140</td>\n",
       "      <td>72d76fde-8bb3-4e00-8c23-a032dfed738c</td>\n",
       "      <td>Oct</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-02 14:30:46 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>17302761</td>\n",
       "      <td>2053013553853497655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.40</td>\n",
       "      <td>541312140</td>\n",
       "      <td>bda25b1a-8844-40ec-b430-a704ab39e9d5</td>\n",
       "      <td>Oct</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-10-05 14:10:39 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>17700454</td>\n",
       "      <td>2053013558861496931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lumene</td>\n",
       "      <td>19.00</td>\n",
       "      <td>541312140</td>\n",
       "      <td>58c59c3e-da37-4a57-8c04-f85e1e4ee77f</td>\n",
       "      <td>Oct</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-10-05 14:11:38 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>17700020</td>\n",
       "      <td>2053013558861496931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>payot</td>\n",
       "      <td>83.58</td>\n",
       "      <td>541312140</td>\n",
       "      <td>23fb14a1-9fd3-4e35-a729-bfaa64f4e875</td>\n",
       "      <td>Oct</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               event_time event_type  product_id   \n",
       "0           0  2019-10-01 00:00:00 UTC       view    44600062  \\\n",
       "1           1  2019-10-01 00:00:15 UTC       view    44600062   \n",
       "2           2  2019-10-02 14:30:46 UTC       view    17302761   \n",
       "3           3  2019-10-05 14:10:39 UTC       view    17700454   \n",
       "4           4  2019-10-05 14:11:38 UTC       view    17700020   \n",
       "\n",
       "           category_id category_code     brand  price    user_id   \n",
       "0  2103807459595387724           NaN  shiseido  35.79  541312140  \\\n",
       "1  2103807459595387724           NaN  shiseido  35.79  541312140   \n",
       "2  2053013553853497655           NaN       NaN  73.40  541312140   \n",
       "3  2053013558861496931           NaN    lumene  19.00  541312140   \n",
       "4  2053013558861496931           NaN     payot  83.58  541312140   \n",
       "\n",
       "                           user_session month  Churn_1  Churn_2  \n",
       "0  72d76fde-8bb3-4e00-8c23-a032dfed738c   Oct      1.0      1.0  \n",
       "1  72d76fde-8bb3-4e00-8c23-a032dfed738c   Oct      1.0      1.0  \n",
       "2  bda25b1a-8844-40ec-b430-a704ab39e9d5   Oct      1.0      1.0  \n",
       "3  58c59c3e-da37-4a57-8c04-f85e1e4ee77f   Oct      1.0      1.0  \n",
       "4  23fb14a1-9fd3-4e35-a729-bfaa64f4e875   Oct      1.0      1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3b51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['event_time', 'product_id', 'category_code', 'user_id', 'user_session', 'month','category_id'], axis=1, inplace = True)\n",
    "df.drop(['Unnamed: 0', 'event_time', 'product_id', 'category_code', 'user_id', 'user_session', 'month'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39123a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14320508 entries, 0 to 14320507\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   event_type   object \n",
      " 1   category_id  int64  \n",
      " 2   brand        object \n",
      " 3   price        float64\n",
      " 4   Churn_1      float64\n",
      " 5   Churn_2      float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 655.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display info about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66741f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3296"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['brand'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5c88cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_type           0\n",
       "category_id          0\n",
       "brand          2071421\n",
       "price                0\n",
       "Churn_1              1\n",
       "Churn_2              1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a711e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_null_values(df, column_name ='brand', replace_with='other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3dcdcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_null_values(df, column_name = 'Churn_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e287d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_null_values(df, column_name = 'Churn_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4808585b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event_type     0\n",
       "category_id    0\n",
       "brand          0\n",
       "price          0\n",
       "Churn_1        0\n",
       "Churn_2        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nulls again\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f690296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hotencoding categorical variables\n",
    "le = LabelEncoder()\n",
    "le_cat_id = le.fit_transform(df['category_id'])\n",
    "le_brand = le.fit_transform(df['brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3860325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "babd8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num['category_id'] = le_cat_id\n",
    "df_num['brand'] = le_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a120a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd.get_dummies(df_num, drop_first = True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe133a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>Churn_1</th>\n",
       "      <th>Churn_2</th>\n",
       "      <th>event_type_purchase</th>\n",
       "      <th>event_type_view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>448</td>\n",
       "      <td>2656</td>\n",
       "      <td>35.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>448</td>\n",
       "      <td>2656</td>\n",
       "      <td>35.79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>2238</td>\n",
       "      <td>73.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>155</td>\n",
       "      <td>1766</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155</td>\n",
       "      <td>2276</td>\n",
       "      <td>83.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  brand  price  Churn_1  Churn_2  event_type_purchase   \n",
       "0          448   2656  35.79      1.0      1.0                    0  \\\n",
       "1          448   2656  35.79      1.0      1.0                    0   \n",
       "2           39   2238  73.40      1.0      1.0                    0   \n",
       "3          155   1766  19.00      1.0      1.0                    0   \n",
       "4          155   2276  83.58      1.0      1.0                    0   \n",
       "\n",
       "   event_type_view  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73230f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the data frame where we had created dummy variables\n",
    "y = df_num['Churn_1'].values\n",
    "X = df_num.drop(columns = ['Churn_1', 'Churn_2'])\n",
    "\n",
    "# Scaling all the variables to a range of 0 to 1\n",
    "features = X.columns.values\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "scaler.fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X))\n",
    "X.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bbd9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train & Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60539f",
   "metadata": {},
   "source": [
    "## **Machine learning models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0670f5b",
   "metadata": {},
   "source": [
    "### **AdaBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8d6d5b",
   "metadata": {},
   "source": [
    "AdaBoost is a boosting algorithm that also works on the principle of the stagewise addition method where multiple weak learners are used for getting strong learners. \n",
    "\n",
    "The value of the alpha parameter, in this case, will be indirectly proportional to the error of the weak learner, Unlike Gradient Boosting in XGBoost, the alpha parameter calculated is related to the errors of the weak learner, here the value of the alpha parameter will be indirectly proportional to the error of the weak learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8225a53",
   "metadata": {},
   "source": [
    "**Here are the hyperparameters:**\n",
    "\n",
    "**estimatorobject**, default=None\n",
    "The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n",
    "\n",
    "**n_estimatorsint**, default=50\n",
    "The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. Values must be in the range [1, inf).\n",
    "\n",
    "**learning_ratefloat**, default=1.0\n",
    "Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters. Values must be in the range (0.0, inf).\n",
    "\n",
    "**algorithm**{â€˜SAMMEâ€™, â€˜SAMME.Râ€™}, default=â€™SAMME.Râ€™\n",
    "If â€˜SAMME.Râ€™ then use the SAMME.R real boosting algorithm. estimator must support calculation of class probabilities. If â€˜SAMMEâ€™ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.\n",
    "\n",
    "**random_stateint, RandomState instance or None**, default=None\n",
    "Controls the random seed given at each estimator at each boosting iteration. Thus, it is only used when estimator exposes a random_state. Pass an int for reproducible output across multiple function calls. See Glossary.\n",
    "\n",
    "**base_estimatorobject**, default=None\n",
    "The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca318ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model_2= AdaBoostClassifier()\n",
    "result_2= model_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f983ba9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6588089390671142\n"
     ]
    }
   ],
   "source": [
    "prediction_test= model_2.predict(X_test)\n",
    "\n",
    "# Print the prediction accuracy\n",
    "print (metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65751c8",
   "metadata": {},
   "source": [
    "###  **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9f040",
   "metadata": {},
   "source": [
    "Several names for this approach include stochastic gradient boosting, gradient boosting machines, multiple additive regression trees, and gradient boosting.\n",
    "\n",
    "Similar to random forest, the Gradient Boosting Decision Trees (GBDT) decision tree ensemble learning algorithm combines different machine learning algorithms to create a more accurate model. Using random bootstrap samples of the data set, random forest is used to construct entire decision trees concurrently.\n",
    "\n",
    "For tasks including regression, classification, and ranking, it is the best machine learning library.\n",
    "\n",
    "**It has the following features:**\n",
    "\n",
    "- Parallel tree boosting.\n",
    "\n",
    "\n",
    "- Regularization.\n",
    "\n",
    "\n",
    "- Built-in features to manage missing data.\n",
    "\n",
    "\n",
    "- At each iteration, the user is able to perform a cross-validation.\n",
    "\n",
    "\n",
    "- In small to medium datasets, it performs well.\n",
    "\n",
    "\n",
    "- It is made to be extremely effective, adaptable, and portable.\n",
    "\n",
    "\n",
    "- To efficiently handle weighted data, it has a distributed weighted quantile sketch process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833d6db",
   "metadata": {},
   "source": [
    "The XGBoost classifier has different hyperparameters that are used to build models. Some of these will be used to improve the model and score.\n",
    "\n",
    "**Here are the hyperparameters:**\n",
    "\n",
    "\n",
    "* **learning_rate:** Learning rate reduces each tree's contribution by learning rate. Between learning rate and n estimators, there is a trade-off.\n",
    "\n",
    "\n",
    "* **n_estimators:** The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "\n",
    "\n",
    "* **subsample:** The percentage of samples that will be used to fit particular base learners. Stochastic Gradient Boosting occurs when the value is less than 1.0. The parameter n estimators interacts with subsample. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "\n",
    "\n",
    "* **colsample_bytree:** Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "\n",
    "*  **nthread:** Number of threads to use for loading data when parallelization is applicable. If -1, uses maximum threads available on the system.\n",
    "\n",
    "\n",
    "* **objective:** Specify the learning task and the corresponding learning objective or a custom objective function to be used.\n",
    "\n",
    "\n",
    "* **silent:** Whether print messages during construction.\n",
    "\n",
    "\n",
    "* **random_state:** Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "895e9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Algorithm\n",
    "from xgboost import XGBClassifier\n",
    "model_3= XGBClassifier()\n",
    "result_3= model_3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98af9f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6598952597824146\n"
     ]
    }
   ],
   "source": [
    "prediction_test = model_3.predict(X_test)\n",
    "\n",
    "# Print the prediction accuracy\n",
    "print (metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b5006",
   "metadata": {},
   "source": [
    "### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a25655",
   "metadata": {},
   "source": [
    "Logistic regression is used to handle the classification problems.\n",
    "\n",
    "It is used in statistical software to understand the relationship between the dependent variable and one or more independent variables by estimating probabilities using a logistic regression equation.  \n",
    "\n",
    "It is often used for predictive analytics and modeling, and extends to applications in machine learning. Logistic regression is easier to implement, interpret, and very efficient to train. \n",
    "\n",
    "\n",
    "**There are three main types of logistic regression:**\n",
    "\n",
    " * **Binary regression** deals with two possible values, essentially: yes or no. \n",
    " \n",
    " \n",
    " * **Multinomial logistic regression** deals with three or more values.\n",
    " \n",
    " \n",
    " * **ordinal logistic regression** deals with three or more classes in a predetermined order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c18c13",
   "metadata": {},
   "source": [
    "To develop a model, the Logistic Regression classifier contains a lot of hyperparameters. I'll use some of them to assist us enhance the model and score.\n",
    "\n",
    "**The hyperparameters are:**\n",
    "* **penalty:** Used to specify the norm used in the penalization. The newton-cg and lbfgs solvers support only l2 penalties.\n",
    "   * `'none':` no penalty is added;\n",
    "   * `'l2':` add a L2 penalty term and it is the default choice;\n",
    "   * `'l1':` add a L1 penalty term;\n",
    "   * `'elasticnet':` both L1 and L2 penalty terms are added.\n",
    "\n",
    "\n",
    "* **C:** Inverse of regularization strength.\n",
    "\n",
    "\n",
    "* **solver:** *(â€˜newton-cgâ€™, â€˜lbfgsâ€™, â€˜liblinearâ€™, â€˜sagâ€™, â€˜sagaâ€™)* use in the optimization problem. Default is â€˜lbfgsâ€™.\n",
    "  * `For small datasets, â€˜liblinearâ€™` is a good choice, whereas â€˜sagâ€™ and â€˜sagaâ€™ are faster for large ones;\n",
    "  * `For multiclass problems,` only â€˜newton-cgâ€™, â€˜sagâ€™, â€˜sagaâ€™ and â€˜lbfgsâ€™ handle multinomial loss;\n",
    "  * `â€˜liblinearâ€™` is limited to one-versus-rest schemes.\n",
    "  \n",
    "  \n",
    "* **random_state:** Used when solver == â€˜sagâ€™, â€˜sagaâ€™ or â€˜liblinearâ€™ to shuffle the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11211241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_5= LogisticRegression()\n",
    "result_5 = model_5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3d11044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6567605948857036\n"
     ]
    }
   ],
   "source": [
    "prediction_test = model_5.predict(X_test)\n",
    "\n",
    "# Print the prediction accuracy\n",
    "print (metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3dea8",
   "metadata": {},
   "source": [
    "### **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3193eb7",
   "metadata": {},
   "source": [
    "A decision tree is a flowchart-like tree structure where each internal node denotes the feature, branches denote the rules and the leaf nodes denote the result of the algorithm.\n",
    "\n",
    "It is a versatile supervised machine-learning algorithm, which is used for both classification and regression problems.\n",
    "\n",
    "It is one of the very powerful algorithms. And it is also used in Random Forest to train on different subsets of training data, which makes random forest one of the most powerful algorithms in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6834eb",
   "metadata": {},
   "source": [
    "**Here are the hyperparameters:**\n",
    "\n",
    "- **Root Node:** It is the topmost node in the tree,  which represents the complete dataset. It is the starting point of the decision-making process.\n",
    "\n",
    "- **Decision/Internal Node:** A node that symbolizes a choice regarding an input feature. Branching off of internal nodes connects them to leaf nodes or other internal nodes.\n",
    "\n",
    "- **Leaf/Terminal Node:** A node without any child nodes that indicates a class label or a numerical value.\n",
    "\n",
    "- **Splitting:** The process of splitting a node into two or more sub-nodes using a split criterion and a selected feature.\n",
    "\n",
    "- **Branch/Sub-Tree:** A subsection of the decision tree starts at an internal node and ends at the leaf nodes.\n",
    "\n",
    "- **Parent Node:** The node that divides into one or more child nodes.\n",
    "\n",
    "- **Child Node:** The nodes that emerge when a parent node is split.\n",
    "\n",
    "- **Impurity:** A measurement of the target variableâ€™s homogeneity in a subset of data. It refers to the degree of randomness or uncertainty in a set of examples. The Gini index and entropy are two commonly used impurity measurements in decision trees for classifications task. \n",
    "\n",
    "- **Variance:** Variance measures how much the predicted and the target variables vary in different samples of a dataset. It is used for regression problems in decision trees. Mean squared error, Mean Absolute Error, friedman_mse, or Half Poisson deviance are used to measure the variance for the regression tasks in the decision tree.\n",
    "\n",
    "- **Information Gain:** Information gain is a measure of the reduction in impurity achieved by splitting a dataset on a particular feature in a decision tree. The splitting criterion is determined by the feature that offers the greatest information gain, It is used to determine the most informative feature to split on at each node of the tree, with the goal of creating pure subsets.\n",
    "\n",
    "- **Pruning:** The process of removing branches from the tree that do not provide any additional information or lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "193a0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_6= DecisionTreeClassifier()\n",
    "result_6 = model_6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20816f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.662608617523631\n"
     ]
    }
   ],
   "source": [
    "prediction_test = model_6.predict(X_test)\n",
    "\n",
    "# Print the prediction accuracy\n",
    "print (metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101756b",
   "metadata": {},
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811806b",
   "metadata": {},
   "source": [
    "A random forest is an ensemble classifier that estimates based on the combination of different decision trees. Effectively, it fits a number of decision tree classifiers on various subsamples of the dataset. Also, each tree in the forest built on a random best subset of features. Finally, the act of enabling these trees gives us the best subset of features among all the random subsets of features. Random forest is currently one of best performing algorithms for many classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f3865",
   "metadata": {},
   "source": [
    "**It has the following features:**\n",
    "\n",
    "\n",
    "- Diversity: Not all attributes/variables/features are considered while making an individual tree; each tree is different.\n",
    "\n",
    "\n",
    "- Immune to the curse of dimensionality: Since each tree does not consider all the features, the feature space is reduced.\n",
    "\n",
    "\n",
    "- Parallelization: Each tree is created independently out of different data and attributes. This means we can fully use the CPU to build random forests.\n",
    "\n",
    "\n",
    "- Train-Test split: In a random forest, we donâ€™t have to segregate the data for train and test as there will always be 30% of the data which is not seen by the decision tree.\n",
    "\n",
    "\n",
    "- Stability: Stability arises because the result is based on majority voting/ averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05bd17",
   "metadata": {},
   "source": [
    "The Random Forest classifier has different hyperparameters that are used to build models. Some of these will be used to improve the model and score.\n",
    "\n",
    "**Here are the hyperparameters:**\n",
    "\n",
    "\n",
    "- **n_estimators:** Number of trees the algorithm builds before averaging the predictions.\n",
    "\n",
    "\n",
    "- **max_features:** Maximum number of features random forest considers splitting a node.\n",
    "\n",
    "\n",
    "- **mini_sample_leaf:** Determines the minimum number of leaves required to split an internal node.\n",
    "\n",
    "\n",
    "- **criterion:** How to split the node in each tree? (Entropy/Gini impurity/Log Loss)\n",
    "\n",
    "\n",
    "- **max_leaf_nodes:** Maximum leaf nodes in each tree\n",
    "\n",
    "\n",
    "- **n_jobs:** it tells the engine how many processors it is allowed to use. If the value is 1, it can use only one processor, but if the value is -1, there is no limit.\n",
    "\n",
    "\n",
    "- **random_state:** controls randomness of the sample. The model will always produce the same results if it has a definite value of random state and has been given the same hyperparameters and training data.\n",
    "\n",
    "\n",
    "- **oob_score:** OOB means out of the bag. It is a random forest cross-validation method. In this, one-third of the sample is not used to train the data; instead used to evaluate its performance. These samples are called out-of-bag samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b530a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_7= RandomForestClassifier()\n",
    "result_7= model_7.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d74ded3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6634230671021261\n"
     ]
    }
   ],
   "source": [
    "prediction_test = model_7.predict(X_test)\n",
    "\n",
    "# Print the prediction accuracy\n",
    "print (metrics.accuracy_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38bbcc",
   "metadata": {},
   "source": [
    "### **Tuning Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a74447",
   "metadata": {},
   "source": [
    "Hyperparameter tuning (or hyperparameter optimization) is the process of determining the right combination of hyperparameters that maximizes the model performance. It works by running multiple trials in a single training process. Each trial is a complete execution of your training application with values for your chosen hyperparameters, set within the limits you specify.\n",
    "\n",
    "This process once finished will give you the set of hyperparameter values that are best suited for the model to give optimal results. Needless to say, It is an important step in any Machine Learning project since it leads to optimal results for a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fa5e9",
   "metadata": {},
   "source": [
    "\n",
    "**Hyperparameters of Tuning Methods *(Grid Search, Random Search, Bayisen Search)* are:**\n",
    "* **estimator:** *(object)* a scikit-learn model.\n",
    "* **param_grid:** *(dict or list of dictionaries)* This enables searching over any sequence of parameter settings.\n",
    "* **scoring:** *(str, callable, list, tuple or dict)* Strategy to evaluate the performance of the cross-validated model on the test set.\n",
    "* **n_jobs:** *(int)* Number of jobs to run in parallel. \n",
    "  * `None` means 1.\n",
    "  * `-1` means using all processors.\n",
    "* **refit:** *(bool, str, or callable)* Refit an estimator using the best found parameters on the whole dataset.\n",
    "* **cv:** *(int, cross-validation generator or an iterable)* determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "  * None, to use the default 5-fold cross validation.\n",
    "  * integer, to specify the number of folds in a (Stratified)KFold.\n",
    "  * CV splitter.\n",
    "  * An iterable yielding (train, test) splits as arrays of indices.\n",
    "* **verbose:** *(int)* Controls the verbosity (Controll to show messages)\n",
    "  * `>1`: the computation time for each fold and parameter candidate is displayed.\n",
    "  * `>2` : the score is also displayed.\n",
    "  * `>3` : the fold and candidate parameter indexes are also displayed together with the starting time of the computation.\n",
    "* **error_score:** *(â€˜raiseâ€™ or numeric)* Value to assign to the score if an error occurs in estimator fitting.\n",
    "\n",
    "There are different hyperparameter tuning methods, the used ones in this task are:\n",
    "\n",
    "- Grid Search \n",
    "- Random Search\n",
    "- Bayesian Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8a7e7",
   "metadata": {},
   "source": [
    "### **Random Search**\n",
    "\n",
    "Using a specified probability distribution or series of probability distributions, random search methods are stochastic approaches that solely rely on the random sampling of a series of points in the problem's feasible region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b06461dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters of RF model\n",
    "param_grid = { \n",
    "'n_estimators': [25, 50, 100, 150], \n",
    "'max_features': ['sqrt', 'log2', None], \n",
    "'max_depth': [3, 6, 9], \n",
    "'max_leaf_nodes': [3, 6, 9], \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc5b5e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=3, max_features=None, max_leaf_nodes=9,\n",
      "                       n_estimators=25)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning- RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(), param_grid) \n",
    "random_search.fit(X_train, y_train) \n",
    "print(random_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94f530a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6588475782868999\n"
     ]
    }
   ],
   "source": [
    "# Update the model\n",
    "model_random = RandomForestClassifier(max_depth=3, \n",
    "                                      max_features=None, \n",
    "                                      max_leaf_nodes=9, \n",
    "                                      n_estimators=25) \n",
    "model_random.fit(X_train, y_train) \n",
    "pred_test_rand = model_random.predict(X_test) \n",
    "\n",
    "# Print the prediction accuracy\n",
    "print (metrics.accuracy_score(y_test, pred_test_rand ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da333a",
   "metadata": {},
   "source": [
    "## **Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2dbaa",
   "metadata": {},
   "source": [
    "Fom all brevious trials, we found that random forest with accuracy 0.663"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669f396",
   "metadata": {},
   "source": [
    "In our churn prediction model, we undertook the method of self-labeling for eight segments and subsequently trained a variety of machine learning models.\n",
    "Through evaluation, it became that the random forest algorithm was the best trial with acc of 56.78% and acc of 65.89% after the random search.\n",
    "However, the obtained accuracy fell short of our expectations. we recognized that this low accuracy could be attributed to the self-labeling process itself.\n",
    "As such, we made the decision to disregard these results as less valuable in our churn prediction. This highlits the importance of ensuring high-quality, well-labeled data in any predictive modeling task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81d024e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 740 ms, total: 3.13 s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#save dataframe as feather in case our notebook got crashed\n",
    "#feather save column data types\n",
    "import os\n",
    "import pyarrow.feather as feather\n",
    "os.makedirs('tmp', exist_ok=True)  # Make a temp dir for storing the feather file\n",
    "feather.write_feather(df, './tmp/df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38df3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 s, sys: 1.2 s, total: 2.6 s\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#load the feather data cause feather more lightweight\n",
    "df = pd.read_feather('./tmp/df')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
